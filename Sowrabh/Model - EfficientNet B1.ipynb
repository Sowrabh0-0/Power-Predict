{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T15:17:13.141469Z",
     "start_time": "2024-12-15T15:17:09.782757Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import logging\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:17:13.479959Z",
     "start_time": "2024-12-15T15:17:13.149088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acuity_path = \"../Choithram Netralaya Data/acuityvalues.xlsx\"\n",
    "\n",
    "df = pd.read_excel(acuity_path)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns)"
   ],
   "id": "db2840470395b9d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   patient  r sphere  r cylinder  l sphere  l cylinder\n",
       "0        1      0.25       -0.25      0.00       -0.25\n",
       "1        2     -0.50       -0.50      0.00       -0.50\n",
       "2        3      1.75       -0.75      0.00       -0.25\n",
       "3        4      0.25        0.00      0.25        0.00\n",
       "4        5      0.25        0.25      0.25        0.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>r sphere</th>\n",
       "      <th>r cylinder</th>\n",
       "      <th>l sphere</th>\n",
       "      <th>l cylinder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['patient', 'r sphere', 'r cylinder', 'l sphere', 'l cylinder'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:17:14.043848Z",
     "start_time": "2024-12-15T15:17:13.646868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "filtered_df = df.copy()\n",
    "numeric_cols = ['r sphere', 'r cylinder', 'l sphere', 'l cylinder']\n",
    "\n",
    "filtered_df = filtered_df.dropna(subset=numeric_cols)\n",
    "\n",
    "df_patients = set(map(str, filtered_df['patient'].unique()))\n",
    "\n",
    "data_dir = \"../Choithram Netralaya Data/Images\"\n",
    "image_patients = set()\n",
    "if os.path.exists(data_dir):\n",
    "    for folder_name in os.listdir(data_dir):\n",
    "        folder_path = os.path.join(data_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            image_patients.add(folder_name)\n",
    "else:\n",
    "    logging.warning(\"Image directory does not exist!\")\n",
    "\n",
    "all_patients = df_patients.union(image_patients)\n",
    "\n",
    "valid_patients = []\n",
    "logging.info(f\"Total patients in DF (after NaN drop): {len(df_patients)}\")\n",
    "logging.info(f\"Total patients in image directory: {len(image_patients)}\")\n",
    "logging.info(f\"Combined unique patients: {len(all_patients)}\")\n",
    "\n",
    "for patient_id in sorted(all_patients, key=lambda x: int(x)):\n",
    "    if patient_id not in df_patients:\n",
    "        logging.info(f\"Skipping patient {patient_id}: no numeric values found in DataFrame.\")\n",
    "        continue\n",
    "    row = filtered_df.loc[filtered_df['patient'] == int(patient_id)].head(1)\n",
    "    if row.empty:\n",
    "        logging.info(f\"Skipping patient {patient_id}: filtered out due to missing numeric data.\")\n",
    "        continue\n",
    "\n",
    "    left_ir_path = os.path.join(data_dir, patient_id, f\"{patient_id}_LEFT_IR.jpg\")\n",
    "    right_ir_path = os.path.join(data_dir, patient_id, f\"{patient_id}_RIGHT_IR.jpg\")\n",
    "\n",
    "    left_exists = os.path.exists(left_ir_path)\n",
    "    right_exists = os.path.exists(right_ir_path)\n",
    "\n",
    "    if not left_exists and not right_exists:\n",
    "        logging.info(f\"Skipping patient {patient_id}: LEFT_IR and RIGHT_IR images missing.\")\n",
    "        continue\n",
    "    elif not left_exists:\n",
    "        logging.info(f\"Skipping patient {patient_id}: LEFT_IR image missing.\")\n",
    "        continue\n",
    "    elif not right_exists:\n",
    "        logging.info(f\"Skipping patient {patient_id}: RIGHT_IR image missing.\")\n",
    "        continue\n",
    "\n",
    "    logging.info(f\"Including patient {patient_id}: Numeric and IR images available.\")\n",
    "    valid_patients.append(patient_id)\n",
    "\n",
    "valid_patient_ids = set(map(int, valid_patients))\n",
    "filtered_df = filtered_df[filtered_df['patient'].isin(valid_patient_ids)].reset_index(drop=True)\n",
    "\n",
    "logging.info(f\"Number of valid patients after filtering: {len(filtered_df)}\")"
   ],
   "id": "4610d7fa08cdf01d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Total patients in DF (after NaN drop): 302\n",
      "INFO: Total patients in image directory: 343\n",
      "INFO: Combined unique patients: 343\n",
      "INFO: Skipping patient 1: LEFT_IR image missing.\n",
      "INFO: Including patient 2: Numeric and IR images available.\n",
      "INFO: Including patient 3: Numeric and IR images available.\n",
      "INFO: Including patient 4: Numeric and IR images available.\n",
      "INFO: Including patient 5: Numeric and IR images available.\n",
      "INFO: Including patient 6: Numeric and IR images available.\n",
      "INFO: Including patient 7: Numeric and IR images available.\n",
      "INFO: Including patient 8: Numeric and IR images available.\n",
      "INFO: Including patient 9: Numeric and IR images available.\n",
      "INFO: Including patient 10: Numeric and IR images available.\n",
      "INFO: Including patient 11: Numeric and IR images available.\n",
      "INFO: Including patient 12: Numeric and IR images available.\n",
      "INFO: Including patient 13: Numeric and IR images available.\n",
      "INFO: Including patient 14: Numeric and IR images available.\n",
      "INFO: Including patient 15: Numeric and IR images available.\n",
      "INFO: Including patient 16: Numeric and IR images available.\n",
      "INFO: Including patient 17: Numeric and IR images available.\n",
      "INFO: Including patient 18: Numeric and IR images available.\n",
      "INFO: Including patient 19: Numeric and IR images available.\n",
      "INFO: Including patient 20: Numeric and IR images available.\n",
      "INFO: Including patient 21: Numeric and IR images available.\n",
      "INFO: Including patient 22: Numeric and IR images available.\n",
      "INFO: Including patient 23: Numeric and IR images available.\n",
      "INFO: Including patient 24: Numeric and IR images available.\n",
      "INFO: Including patient 25: Numeric and IR images available.\n",
      "INFO: Including patient 26: Numeric and IR images available.\n",
      "INFO: Including patient 27: Numeric and IR images available.\n",
      "INFO: Including patient 28: Numeric and IR images available.\n",
      "INFO: Including patient 29: Numeric and IR images available.\n",
      "INFO: Including patient 30: Numeric and IR images available.\n",
      "INFO: Including patient 31: Numeric and IR images available.\n",
      "INFO: Including patient 32: Numeric and IR images available.\n",
      "INFO: Including patient 33: Numeric and IR images available.\n",
      "INFO: Including patient 34: Numeric and IR images available.\n",
      "INFO: Including patient 35: Numeric and IR images available.\n",
      "INFO: Including patient 36: Numeric and IR images available.\n",
      "INFO: Including patient 37: Numeric and IR images available.\n",
      "INFO: Including patient 38: Numeric and IR images available.\n",
      "INFO: Including patient 39: Numeric and IR images available.\n",
      "INFO: Including patient 40: Numeric and IR images available.\n",
      "INFO: Including patient 41: Numeric and IR images available.\n",
      "INFO: Including patient 42: Numeric and IR images available.\n",
      "INFO: Including patient 43: Numeric and IR images available.\n",
      "INFO: Including patient 44: Numeric and IR images available.\n",
      "INFO: Including patient 45: Numeric and IR images available.\n",
      "INFO: Skipping patient 46: no numeric values found in DataFrame.\n",
      "INFO: Including patient 47: Numeric and IR images available.\n",
      "INFO: Including patient 48: Numeric and IR images available.\n",
      "INFO: Including patient 49: Numeric and IR images available.\n",
      "INFO: Including patient 50: Numeric and IR images available.\n",
      "INFO: Including patient 51: Numeric and IR images available.\n",
      "INFO: Including patient 52: Numeric and IR images available.\n",
      "INFO: Skipping patient 53: no numeric values found in DataFrame.\n",
      "INFO: Including patient 54: Numeric and IR images available.\n",
      "INFO: Including patient 55: Numeric and IR images available.\n",
      "INFO: Including patient 56: Numeric and IR images available.\n",
      "INFO: Including patient 57: Numeric and IR images available.\n",
      "INFO: Including patient 58: Numeric and IR images available.\n",
      "INFO: Including patient 59: Numeric and IR images available.\n",
      "INFO: Including patient 60: Numeric and IR images available.\n",
      "INFO: Including patient 61: Numeric and IR images available.\n",
      "INFO: Including patient 62: Numeric and IR images available.\n",
      "INFO: Including patient 63: Numeric and IR images available.\n",
      "INFO: Including patient 64: Numeric and IR images available.\n",
      "INFO: Including patient 65: Numeric and IR images available.\n",
      "INFO: Including patient 66: Numeric and IR images available.\n",
      "INFO: Including patient 67: Numeric and IR images available.\n",
      "INFO: Including patient 68: Numeric and IR images available.\n",
      "INFO: Including patient 69: Numeric and IR images available.\n",
      "INFO: Including patient 70: Numeric and IR images available.\n",
      "INFO: Including patient 71: Numeric and IR images available.\n",
      "INFO: Including patient 72: Numeric and IR images available.\n",
      "INFO: Including patient 73: Numeric and IR images available.\n",
      "INFO: Including patient 74: Numeric and IR images available.\n",
      "INFO: Including patient 75: Numeric and IR images available.\n",
      "INFO: Including patient 76: Numeric and IR images available.\n",
      "INFO: Including patient 77: Numeric and IR images available.\n",
      "INFO: Including patient 78: Numeric and IR images available.\n",
      "INFO: Skipping patient 79: no numeric values found in DataFrame.\n",
      "INFO: Including patient 80: Numeric and IR images available.\n",
      "INFO: Including patient 81: Numeric and IR images available.\n",
      "INFO: Including patient 82: Numeric and IR images available.\n",
      "INFO: Including patient 83: Numeric and IR images available.\n",
      "INFO: Including patient 84: Numeric and IR images available.\n",
      "INFO: Including patient 85: Numeric and IR images available.\n",
      "INFO: Including patient 86: Numeric and IR images available.\n",
      "INFO: Including patient 87: Numeric and IR images available.\n",
      "INFO: Including patient 88: Numeric and IR images available.\n",
      "INFO: Including patient 89: Numeric and IR images available.\n",
      "INFO: Skipping patient 90: no numeric values found in DataFrame.\n",
      "INFO: Including patient 91: Numeric and IR images available.\n",
      "INFO: Including patient 92: Numeric and IR images available.\n",
      "INFO: Including patient 93: Numeric and IR images available.\n",
      "INFO: Including patient 94: Numeric and IR images available.\n",
      "INFO: Including patient 95: Numeric and IR images available.\n",
      "INFO: Including patient 96: Numeric and IR images available.\n",
      "INFO: Including patient 97: Numeric and IR images available.\n",
      "INFO: Including patient 98: Numeric and IR images available.\n",
      "INFO: Including patient 99: Numeric and IR images available.\n",
      "INFO: Including patient 100: Numeric and IR images available.\n",
      "INFO: Including patient 101: Numeric and IR images available.\n",
      "INFO: Including patient 102: Numeric and IR images available.\n",
      "INFO: Skipping patient 103: no numeric values found in DataFrame.\n",
      "INFO: Including patient 104: Numeric and IR images available.\n",
      "INFO: Including patient 105: Numeric and IR images available.\n",
      "INFO: Including patient 106: Numeric and IR images available.\n",
      "INFO: Skipping patient 107: no numeric values found in DataFrame.\n",
      "INFO: Including patient 108: Numeric and IR images available.\n",
      "INFO: Including patient 109: Numeric and IR images available.\n",
      "INFO: Including patient 110: Numeric and IR images available.\n",
      "INFO: Including patient 111: Numeric and IR images available.\n",
      "INFO: Including patient 112: Numeric and IR images available.\n",
      "INFO: Skipping patient 113: no numeric values found in DataFrame.\n",
      "INFO: Including patient 114: Numeric and IR images available.\n",
      "INFO: Skipping patient 115: no numeric values found in DataFrame.\n",
      "INFO: Including patient 116: Numeric and IR images available.\n",
      "INFO: Including patient 117: Numeric and IR images available.\n",
      "INFO: Skipping patient 118: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 119: no numeric values found in DataFrame.\n",
      "INFO: Including patient 120: Numeric and IR images available.\n",
      "INFO: Including patient 121: Numeric and IR images available.\n",
      "INFO: Skipping patient 122: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 123: no numeric values found in DataFrame.\n",
      "INFO: Including patient 124: Numeric and IR images available.\n",
      "INFO: Including patient 125: Numeric and IR images available.\n",
      "INFO: Skipping patient 126: no numeric values found in DataFrame.\n",
      "INFO: Including patient 127: Numeric and IR images available.\n",
      "INFO: Including patient 128: Numeric and IR images available.\n",
      "INFO: Skipping patient 129: no numeric values found in DataFrame.\n",
      "INFO: Including patient 130: Numeric and IR images available.\n",
      "INFO: Including patient 131: Numeric and IR images available.\n",
      "INFO: Including patient 132: Numeric and IR images available.\n",
      "INFO: Including patient 133: Numeric and IR images available.\n",
      "INFO: Including patient 134: Numeric and IR images available.\n",
      "INFO: Including patient 135: Numeric and IR images available.\n",
      "INFO: Including patient 136: Numeric and IR images available.\n",
      "INFO: Including patient 137: Numeric and IR images available.\n",
      "INFO: Including patient 138: Numeric and IR images available.\n",
      "INFO: Including patient 139: Numeric and IR images available.\n",
      "INFO: Including patient 140: Numeric and IR images available.\n",
      "INFO: Including patient 141: Numeric and IR images available.\n",
      "INFO: Skipping patient 142: no numeric values found in DataFrame.\n",
      "INFO: Including patient 143: Numeric and IR images available.\n",
      "INFO: Including patient 144: Numeric and IR images available.\n",
      "INFO: Skipping patient 145: no numeric values found in DataFrame.\n",
      "INFO: Including patient 146: Numeric and IR images available.\n",
      "INFO: Including patient 147: Numeric and IR images available.\n",
      "INFO: Including patient 148: Numeric and IR images available.\n",
      "INFO: Including patient 149: Numeric and IR images available.\n",
      "INFO: Including patient 150: Numeric and IR images available.\n",
      "INFO: Including patient 151: Numeric and IR images available.\n",
      "INFO: Including patient 152: Numeric and IR images available.\n",
      "INFO: Including patient 153: Numeric and IR images available.\n",
      "INFO: Skipping patient 154: no numeric values found in DataFrame.\n",
      "INFO: Including patient 155: Numeric and IR images available.\n",
      "INFO: Including patient 156: Numeric and IR images available.\n",
      "INFO: Including patient 157: Numeric and IR images available.\n",
      "INFO: Including patient 158: Numeric and IR images available.\n",
      "INFO: Including patient 159: Numeric and IR images available.\n",
      "INFO: Skipping patient 160: no numeric values found in DataFrame.\n",
      "INFO: Including patient 161: Numeric and IR images available.\n",
      "INFO: Including patient 162: Numeric and IR images available.\n",
      "INFO: Including patient 163: Numeric and IR images available.\n",
      "INFO: Including patient 164: Numeric and IR images available.\n",
      "INFO: Including patient 165: Numeric and IR images available.\n",
      "INFO: Including patient 166: Numeric and IR images available.\n",
      "INFO: Including patient 167: Numeric and IR images available.\n",
      "INFO: Including patient 168: Numeric and IR images available.\n",
      "INFO: Including patient 169: Numeric and IR images available.\n",
      "INFO: Including patient 170: Numeric and IR images available.\n",
      "INFO: Including patient 171: Numeric and IR images available.\n",
      "INFO: Including patient 172: Numeric and IR images available.\n",
      "INFO: Including patient 173: Numeric and IR images available.\n",
      "INFO: Including patient 174: Numeric and IR images available.\n",
      "INFO: Including patient 175: Numeric and IR images available.\n",
      "INFO: Skipping patient 176: no numeric values found in DataFrame.\n",
      "INFO: Including patient 177: Numeric and IR images available.\n",
      "INFO: Including patient 178: Numeric and IR images available.\n",
      "INFO: Including patient 179: Numeric and IR images available.\n",
      "INFO: Including patient 180: Numeric and IR images available.\n",
      "INFO: Including patient 181: Numeric and IR images available.\n",
      "INFO: Including patient 182: Numeric and IR images available.\n",
      "INFO: Skipping patient 183: no numeric values found in DataFrame.\n",
      "INFO: Including patient 184: Numeric and IR images available.\n",
      "INFO: Including patient 185: Numeric and IR images available.\n",
      "INFO: Including patient 186: Numeric and IR images available.\n",
      "INFO: Skipping patient 187: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 188: RIGHT_IR image missing.\n",
      "INFO: Including patient 189: Numeric and IR images available.\n",
      "INFO: Including patient 190: Numeric and IR images available.\n",
      "INFO: Including patient 191: Numeric and IR images available.\n",
      "INFO: Including patient 192: Numeric and IR images available.\n",
      "INFO: Including patient 193: Numeric and IR images available.\n",
      "INFO: Including patient 194: Numeric and IR images available.\n",
      "INFO: Including patient 195: Numeric and IR images available.\n",
      "INFO: Including patient 196: Numeric and IR images available.\n",
      "INFO: Including patient 197: Numeric and IR images available.\n",
      "INFO: Skipping patient 198: no numeric values found in DataFrame.\n",
      "INFO: Including patient 199: Numeric and IR images available.\n",
      "INFO: Including patient 200: Numeric and IR images available.\n",
      "INFO: Including patient 201: Numeric and IR images available.\n",
      "INFO: Skipping patient 202: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 203: no numeric values found in DataFrame.\n",
      "INFO: Including patient 204: Numeric and IR images available.\n",
      "INFO: Including patient 205: Numeric and IR images available.\n",
      "INFO: Skipping patient 206: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 207: no numeric values found in DataFrame.\n",
      "INFO: Including patient 210: Numeric and IR images available.\n",
      "INFO: Including patient 211: Numeric and IR images available.\n",
      "INFO: Including patient 212: Numeric and IR images available.\n",
      "INFO: Skipping patient 213: no numeric values found in DataFrame.\n",
      "INFO: Including patient 224: Numeric and IR images available.\n",
      "INFO: Including patient 225: Numeric and IR images available.\n",
      "INFO: Including patient 226: Numeric and IR images available.\n",
      "INFO: Including patient 227: Numeric and IR images available.\n",
      "INFO: Including patient 228: Numeric and IR images available.\n",
      "INFO: Including patient 229: Numeric and IR images available.\n",
      "INFO: Including patient 230: Numeric and IR images available.\n",
      "INFO: Including patient 231: Numeric and IR images available.\n",
      "INFO: Including patient 232: Numeric and IR images available.\n",
      "INFO: Including patient 233: Numeric and IR images available.\n",
      "INFO: Including patient 234: Numeric and IR images available.\n",
      "INFO: Including patient 235: Numeric and IR images available.\n",
      "INFO: Including patient 236: Numeric and IR images available.\n",
      "INFO: Including patient 237: Numeric and IR images available.\n",
      "INFO: Including patient 238: Numeric and IR images available.\n",
      "INFO: Including patient 239: Numeric and IR images available.\n",
      "INFO: Including patient 240: Numeric and IR images available.\n",
      "INFO: Including patient 241: Numeric and IR images available.\n",
      "INFO: Including patient 242: Numeric and IR images available.\n",
      "INFO: Skipping patient 243: no numeric values found in DataFrame.\n",
      "INFO: Including patient 244: Numeric and IR images available.\n",
      "INFO: Including patient 245: Numeric and IR images available.\n",
      "INFO: Skipping patient 246: no numeric values found in DataFrame.\n",
      "INFO: Including patient 247: Numeric and IR images available.\n",
      "INFO: Including patient 248: Numeric and IR images available.\n",
      "INFO: Skipping patient 249: RIGHT_IR image missing.\n",
      "INFO: Skipping patient 250: RIGHT_IR image missing.\n",
      "INFO: Including patient 251: Numeric and IR images available.\n",
      "INFO: Including patient 252: Numeric and IR images available.\n",
      "INFO: Including patient 253: Numeric and IR images available.\n",
      "INFO: Including patient 254: Numeric and IR images available.\n",
      "INFO: Including patient 255: Numeric and IR images available.\n",
      "INFO: Including patient 256: Numeric and IR images available.\n",
      "INFO: Including patient 257: Numeric and IR images available.\n",
      "INFO: Including patient 258: Numeric and IR images available.\n",
      "INFO: Including patient 259: Numeric and IR images available.\n",
      "INFO: Including patient 260: Numeric and IR images available.\n",
      "INFO: Including patient 261: Numeric and IR images available.\n",
      "INFO: Including patient 263: Numeric and IR images available.\n",
      "INFO: Including patient 264: Numeric and IR images available.\n",
      "INFO: Including patient 265: Numeric and IR images available.\n",
      "INFO: Including patient 266: Numeric and IR images available.\n",
      "INFO: Including patient 267: Numeric and IR images available.\n",
      "INFO: Including patient 268: Numeric and IR images available.\n",
      "INFO: Including patient 269: Numeric and IR images available.\n",
      "INFO: Including patient 270: Numeric and IR images available.\n",
      "INFO: Including patient 271: Numeric and IR images available.\n",
      "INFO: Including patient 272: Numeric and IR images available.\n",
      "INFO: Including patient 273: Numeric and IR images available.\n",
      "INFO: Including patient 274: Numeric and IR images available.\n",
      "INFO: Including patient 275: Numeric and IR images available.\n",
      "INFO: Including patient 276: Numeric and IR images available.\n",
      "INFO: Including patient 277: Numeric and IR images available.\n",
      "INFO: Including patient 278: Numeric and IR images available.\n",
      "INFO: Including patient 279: Numeric and IR images available.\n",
      "INFO: Including patient 280: Numeric and IR images available.\n",
      "INFO: Skipping patient 281: no numeric values found in DataFrame.\n",
      "INFO: Including patient 282: Numeric and IR images available.\n",
      "INFO: Including patient 283: Numeric and IR images available.\n",
      "INFO: Skipping patient 284: LEFT_IR image missing.\n",
      "INFO: Including patient 285: Numeric and IR images available.\n",
      "INFO: Including patient 286: Numeric and IR images available.\n",
      "INFO: Including patient 287: Numeric and IR images available.\n",
      "INFO: Skipping patient 288: no numeric values found in DataFrame.\n",
      "INFO: Including patient 289: Numeric and IR images available.\n",
      "INFO: Including patient 290: Numeric and IR images available.\n",
      "INFO: Including patient 291: Numeric and IR images available.\n",
      "INFO: Skipping patient 292: RIGHT_IR image missing.\n",
      "INFO: Including patient 293: Numeric and IR images available.\n",
      "INFO: Including patient 294: Numeric and IR images available.\n",
      "INFO: Including patient 295: Numeric and IR images available.\n",
      "INFO: Including patient 296: Numeric and IR images available.\n",
      "INFO: Skipping patient 297: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 298: no numeric values found in DataFrame.\n",
      "INFO: Including patient 299: Numeric and IR images available.\n",
      "INFO: Including patient 300: Numeric and IR images available.\n",
      "INFO: Including patient 301: Numeric and IR images available.\n",
      "INFO: Including patient 302: Numeric and IR images available.\n",
      "INFO: Including patient 303: Numeric and IR images available.\n",
      "INFO: Skipping patient 304: LEFT_IR image missing.\n",
      "INFO: Including patient 305: Numeric and IR images available.\n",
      "INFO: Skipping patient 306: no numeric values found in DataFrame.\n",
      "INFO: Including patient 307: Numeric and IR images available.\n",
      "INFO: Including patient 308: Numeric and IR images available.\n",
      "INFO: Including patient 309: Numeric and IR images available.\n",
      "INFO: Skipping patient 310: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 311: no numeric values found in DataFrame.\n",
      "INFO: Including patient 316: Numeric and IR images available.\n",
      "INFO: Including patient 317: Numeric and IR images available.\n",
      "INFO: Including patient 318: Numeric and IR images available.\n",
      "INFO: Including patient 319: Numeric and IR images available.\n",
      "INFO: Including patient 320: Numeric and IR images available.\n",
      "INFO: Including patient 321: Numeric and IR images available.\n",
      "INFO: Including patient 322: Numeric and IR images available.\n",
      "INFO: Including patient 323: Numeric and IR images available.\n",
      "INFO: Including patient 324: Numeric and IR images available.\n",
      "INFO: Including patient 325: Numeric and IR images available.\n",
      "INFO: Including patient 326: Numeric and IR images available.\n",
      "INFO: Including patient 327: Numeric and IR images available.\n",
      "INFO: Including patient 328: Numeric and IR images available.\n",
      "INFO: Including patient 329: Numeric and IR images available.\n",
      "INFO: Including patient 330: Numeric and IR images available.\n",
      "INFO: Including patient 331: Numeric and IR images available.\n",
      "INFO: Including patient 332: Numeric and IR images available.\n",
      "INFO: Including patient 333: Numeric and IR images available.\n",
      "INFO: Including patient 334: Numeric and IR images available.\n",
      "INFO: Including patient 335: Numeric and IR images available.\n",
      "INFO: Including patient 336: Numeric and IR images available.\n",
      "INFO: Including patient 337: Numeric and IR images available.\n",
      "INFO: Including patient 338: Numeric and IR images available.\n",
      "INFO: Including patient 339: Numeric and IR images available.\n",
      "INFO: Including patient 340: Numeric and IR images available.\n",
      "INFO: Including patient 341: Numeric and IR images available.\n",
      "INFO: Including patient 342: Numeric and IR images available.\n",
      "INFO: Skipping patient 343: no numeric values found in DataFrame.\n",
      "INFO: Including patient 344: Numeric and IR images available.\n",
      "INFO: Including patient 345: Numeric and IR images available.\n",
      "INFO: Skipping patient 346: no numeric values found in DataFrame.\n",
      "INFO: Skipping patient 347: no numeric values found in DataFrame.\n",
      "INFO: Including patient 348: Numeric and IR images available.\n",
      "INFO: Including patient 349: Numeric and IR images available.\n",
      "INFO: Including patient 350: Numeric and IR images available.\n",
      "INFO: Including patient 351: Numeric and IR images available.\n",
      "INFO: Skipping patient 352: no numeric values found in DataFrame.\n",
      "INFO: Including patient 353: Numeric and IR images available.\n",
      "INFO: Including patient 354: Numeric and IR images available.\n",
      "INFO: Including patient 355: Numeric and IR images available.\n",
      "INFO: Including patient 356: Numeric and IR images available.\n",
      "INFO: Including patient 357: Numeric and IR images available.\n",
      "INFO: Including patient 358: Numeric and IR images available.\n",
      "INFO: Including patient 359: Numeric and IR images available.\n",
      "INFO: Skipping patient 360: no numeric values found in DataFrame.\n",
      "INFO: Number of valid patients after filtering: 295\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:37:59.195654Z",
     "start_time": "2024-12-15T15:37:59.187825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EyeIRDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transform=None, is_training=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): Data containing patient IDs and numerical features.\n",
    "            data_dir (str): Directory where images are stored.\n",
    "            transform: Image transformations.\n",
    "            is_training (bool): Whether the dataset is for training or testing.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Use patient_id internally for loading images\n",
    "        patient_id = str(int(row['patient']))\n",
    "        left_ir_path = os.path.join(self.data_dir, patient_id, f\"{patient_id}_LEFT_IR.jpg\")\n",
    "        right_ir_path = os.path.join(self.data_dir, patient_id, f\"{patient_id}_RIGHT_IR.jpg\")\n",
    "\n",
    "        try:\n",
    "            left_img = Image.open(left_ir_path).convert(\"RGB\")\n",
    "            right_img = Image.open(right_ir_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, IOError) as e:\n",
    "            print(f\"Error loading image for patient {patient_id}: {e}\")\n",
    "            left_img = Image.new(\"RGB\", (240, 240), (0, 0, 0))\n",
    "            right_img = Image.new(\"RGB\", (240, 240), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            left_img = self.transform(left_img)\n",
    "            right_img = self.transform(right_img)\n",
    "\n",
    "        if self.is_training:\n",
    "            numeric_features = torch.tensor(\n",
    "                [row['r sphere'], row['r cylinder'], row['l sphere'], row['l cylinder']],\n",
    "                dtype=torch.float\n",
    "            )\n",
    "            target = numeric_features.clone()\n",
    "\n",
    "            # Add weights\n",
    "            weights = torch.tensor(\n",
    "                [row['r_weight'], row['c_weight'], row['l_weight'], row['lc_weight']],\n",
    "                dtype=torch.float\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"left_ir\": left_img,\n",
    "                \"right_ir\": right_img,\n",
    "                \"numeric_features\": numeric_features,\n",
    "                \"target\": target,\n",
    "                \"weights\": weights\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"left_ir\": left_img,\n",
    "                \"right_ir\": right_img\n",
    "            }\n"
   ],
   "id": "4bc8a0707f3ed34f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:38:04.139862Z",
     "start_time": "2024-12-15T15:38:04.124195Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c11224d6f1c82cad",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtrain_dataset\u001B[49m:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(sample)\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T05:35:58.536044Z",
     "start_time": "2024-12-15T05:35:58.531431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "data_dir = \"../Choithram Netralaya Data/Images\"\n",
    "dataset = EyeIRDataset(filtered_df, data_dir, transform=transform)"
   ],
   "id": "f8789ca36328ddd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T05:36:08.331098Z",
     "start_time": "2024-12-15T05:36:08.287023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check a single sample from the dataset\n",
    "sample = dataset[0]\n",
    "print(\"Left IR Shape:\", sample['left_ir'].shape)\n",
    "print(\"Right IR Shape:\", sample['right_ir'].shape)\n",
    "print(\"Numeric Features:\", sample['numeric_features'])\n",
    "print(\"Target:\", sample['target'])\n"
   ],
   "id": "c869d3ed30788508",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left IR Shape: torch.Size([3, 224, 224])\n",
      "Right IR Shape: torch.Size([3, 224, 224])\n",
      "Numeric Features: tensor([-0.5000, -0.5000,  0.0000, -0.5000])\n",
      "Target: tensor([-0.5000, -0.5000,  0.0000, -0.5000])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T05:36:53.636742Z",
     "start_time": "2024-12-15T05:36:53.398761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust based on your hardware capabilities\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify a batch from DataLoader\n",
    "for batch in train_loader:\n",
    "    print(\"Left IR Batch Shape:\", batch['left_ir'].shape)\n",
    "    print(\"Right IR Batch Shape:\", batch['right_ir'].shape)\n",
    "    print(\"Numeric Features Batch Shape:\", batch['numeric_features'].shape)\n",
    "    print(\"Target Batch Shape:\", batch['target'].shape)\n",
    "    break"
   ],
   "id": "15beca905ca0bece",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left IR Batch Shape: torch.Size([16, 3, 224, 224])\n",
      "Right IR Batch Shape: torch.Size([16, 3, 224, 224])\n",
      "Numeric Features Batch Shape: torch.Size([16, 4])\n",
      "Target Batch Shape: torch.Size([16, 4])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T10:24:57.775715Z",
     "start_time": "2024-12-15T10:24:52.526439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "\n",
    "# Define the Eye Model with EfficientNet B1\n",
    "class EyeModel(nn.Module):\n",
    "    def __init__(self, output_dim=4):\n",
    "        super(EyeModel, self).__init__()\n",
    "        # Load EfficientNet B1\n",
    "        self.cnn = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V1)\n",
    "        self.cnn.classifier = nn.Identity()  # Remove classifier to extract features\n",
    "\n",
    "        # Fully connected layers for image features\n",
    "        self.img_compress = nn.Sequential(\n",
    "            nn.Linear(1280 * 2, 512),  # 1280 features per image x 2 (left and right IR)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # MLP for numeric features\n",
    "        self.numeric_branch = nn.Sequential(\n",
    "            nn.Linear(4, 64),  # 4 numeric inputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for final prediction\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128),  # Combine image and numeric features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(128, output_dim)  # Output: [r sphere, r cylinder, l sphere, l cylinder]\n",
    "        )\n",
    "\n",
    "    def forward(self, left_ir, right_ir, numeric):\n",
    "        # Image feature extraction\n",
    "        left_feat = self.cnn(left_ir)\n",
    "        right_feat = self.cnn(right_ir)\n",
    "\n",
    "        # Combine and compress image features\n",
    "        combined_img_feat = torch.cat([left_feat, right_feat], dim=1)\n",
    "        combined_img_feat = self.img_compress(combined_img_feat)\n",
    "\n",
    "        # Process numeric features\n",
    "        numeric_feat = self.numeric_branch(numeric)\n",
    "\n",
    "        # Combine all features for final prediction\n",
    "        fused_feat = torch.cat([combined_img_feat, numeric_feat], dim=1)\n",
    "        out = self.fc(fused_feat)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = EyeModel().to(device)\n",
    "print(model)\n"
   ],
   "id": "a91f04468b3bf33f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EyeModel(\n",
      "  (cnn): EfficientNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
      "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Identity()\n",
      "  )\n",
      "  (img_compress): Sequential(\n",
      "    (0): Linear(in_features=2560, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (numeric_branch): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=544, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.6, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T10:25:02.559666Z",
     "start_time": "2024-12-15T10:25:02.518176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "# Define the weights for each feature\n",
    "weights = torch.tensor([0.5, 0.3, 0.1, 0.1]).to(device)  # Adjust these weights as necessary\n",
    "\n",
    "# Custom weighted loss function\n",
    "def weighted_mse_loss(predictions, targets, weights):\n",
    "    mse = (predictions - targets) ** 2\n",
    "    weighted_mse = weights * mse  # Apply weights to each feature\n",
    "    return weighted_mse.mean()\n",
    "\n",
    "# Use the custom loss function in the training loop\n",
    "criterion = lambda preds, targets: weighted_mse_loss(preds, targets, weights)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)  # Adjust T_max based on epochs\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "# Print optimizer and scheduler details for verification\n",
    "print(optimizer)\n",
    "print(scheduler)"
   ],
   "id": "bb8f0ef5fcf10be1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x0000012FCACD9520>\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:33:33.986493Z",
     "start_time": "2024-12-15T15:33:33.769121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Metric calculation function (overall)\n",
    "def calculate_metrics(predictions, targets, weights=None):\n",
    "    \"\"\"\n",
    "    Calculates overall metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R).\n",
    "    Args:\n",
    "        predictions (Tensor): Predicted values from the model.\n",
    "        targets (Tensor): Ground truth values.\n",
    "        weights (Tensor, optional): Weights for weighted metrics.\n",
    "    Returns:\n",
    "        tuple: MAE, RMSE, R for the entire batch.\n",
    "    \"\"\"\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    weights = weights.detach().cpu().numpy() if weights is not None else np.ones_like(targets)\n",
    "\n",
    "    mae = np.mean(weights * np.abs(predictions - targets))\n",
    "    rmse = np.sqrt(np.mean(weights * (predictions - targets) ** 2))\n",
    "    total_variance = np.sum(weights * (targets - np.mean(targets)) ** 2)\n",
    "    explained_variance = np.sum(weights * (predictions - targets) ** 2)\n",
    "    r_squared = 1 - (explained_variance / total_variance) if total_variance != 0 else 0\n",
    "\n",
    "    return mae, rmse, r_squared\n",
    "\n",
    "\n",
    "# Metric calculation function (feature-specific)\n",
    "def calculate_feature_metrics(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculates metrics per output feature: MAE, RMSE, and R.\n",
    "    \"\"\"\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    feature_metrics = {}\n",
    "\n",
    "    for i, feature in enumerate(['r sphere', 'r cylinder', 'l sphere', 'l cylinder']):\n",
    "        pred = predictions[:, i]\n",
    "        tgt = targets[:, i]\n",
    "        mae = np.mean(np.abs(pred - tgt))\n",
    "        rmse = np.sqrt(np.mean((pred - tgt) ** 2))\n",
    "        r_squared = 1 - (np.sum((tgt - pred) ** 2) / np.sum((tgt - np.mean(tgt)) ** 2))\n",
    "        feature_metrics[feature] = {'mae': mae, 'rmse': rmse, 'r2': r_squared}\n",
    "\n",
    "    return feature_metrics\n",
    "\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.01, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01)\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, train_mae, train_rmse, train_r2 = 0, 0, 0, 0\n",
    "    train_feature_metrics = {'r sphere': {'mae': 0, 'rmse': 0, 'r2': 0},\n",
    "                             'r cylinder': {'mae': 0, 'rmse': 0, 'r2': 0},\n",
    "                             'l sphere': {'mae': 0, 'rmse': 0, 'r2': 0},\n",
    "                             'l cylinder': {'mae': 0, 'rmse': 0, 'r2': 0}}\n",
    "\n",
    "    for batch in train_loader:\n",
    "        left_ir = batch['left_ir'].to(device)\n",
    "        right_ir = batch['right_ir'].to(device)\n",
    "        numeric = batch['numeric_features'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        weights = batch['weights'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(left_ir, right_ir, numeric)\n",
    "        loss = criterion(predictions, target, weights)  # Weighted loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        mae, rmse, r2 = calculate_metrics(predictions, target, weights)\n",
    "        train_mae += mae\n",
    "        train_rmse += rmse\n",
    "        train_r2 += r2\n",
    "\n",
    "        # Feature-specific metrics\n",
    "        batch_feature_metrics = calculate_feature_metrics(predictions, target)\n",
    "        for feature in train_feature_metrics:\n",
    "            train_feature_metrics[feature]['mae'] += batch_feature_metrics[feature]['mae']\n",
    "            train_feature_metrics[feature]['rmse'] += batch_feature_metrics[feature]['rmse']\n",
    "            train_feature_metrics[feature]['r2'] += batch_feature_metrics[feature]['r2']\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_mae /= len(train_loader)\n",
    "    train_rmse /= len(train_loader)\n",
    "    train_r2 /= len(train_loader)\n",
    "    for feature in train_feature_metrics:\n",
    "        train_feature_metrics[feature]['mae'] /= len(train_loader)\n",
    "        train_feature_metrics[feature]['rmse'] /= len(train_loader)\n",
    "        train_feature_metrics[feature]['r2'] /= len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_mae, val_rmse, val_r2 = 0, 0, 0, 0\n",
    "    val_feature_metrics = {'r sphere': {'mae': 0, 'rmse': 0, 'r2': 0},\n",
    "                           'r cylinder': {'mae': 0, 'rmse': 0, 'r2': 0},\n",
    "                           'l sphere': {'mae': 0, 'rmse': 0, 'r2': 0},\n",
    "                           'l cylinder': {'mae': 0, 'rmse': 0, 'r2': 0}}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            left_ir = batch['left_ir'].to(device)\n",
    "            right_ir = batch['right_ir'].to(device)\n",
    "            numeric = batch['numeric_features'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            weights = batch['weights'].to(device)\n",
    "\n",
    "            predictions = model(left_ir, right_ir, numeric)\n",
    "            loss = criterion(predictions, target, weights)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            mae, rmse, r2 = calculate_metrics(predictions, target, weights)\n",
    "            val_mae += mae\n",
    "            val_rmse += rmse\n",
    "            val_r2 += r2\n",
    "\n",
    "            # Feature-specific metrics\n",
    "            batch_feature_metrics = calculate_feature_metrics(predictions, target)\n",
    "            for feature in val_feature_metrics:\n",
    "                val_feature_metrics[feature]['mae'] += batch_feature_metrics[feature]['mae']\n",
    "                val_feature_metrics[feature]['rmse'] += batch_feature_metrics[feature]['rmse']\n",
    "                val_feature_metrics[feature]['r2'] += batch_feature_metrics[feature]['r2']\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_mae /= len(test_loader)\n",
    "    val_rmse /= len(test_loader)\n",
    "    val_r2 /= len(test_loader)\n",
    "    for feature in val_feature_metrics:\n",
    "        val_feature_metrics[feature]['mae'] /= len(test_loader)\n",
    "        val_feature_metrics[feature]['rmse'] /= len(test_loader)\n",
    "        val_feature_metrics[feature]['r2'] /= len(test_loader)\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # Print metrics for this epoch\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}], LR: {scheduler.get_last_lr()[0]:.6f}\\n\"\n",
    "        f\"Train Loss: {train_loss:.4f}, MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R: {train_r2:.4f}\\n\"\n",
    "        f\"Val Loss: {val_loss:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R: {val_r2:.4f}\\n\"\n",
    "    )\n",
    "    print(\"Feature-Specific Metrics (Train):\")\n",
    "    for feature, metrics in train_feature_metrics.items():\n",
    "        print(f\"  {feature}: MAE: {metrics['mae']:.4f}, RMSE: {metrics['rmse']:.4f}, R: {metrics['r2']:.4f}\")\n",
    "    print(\"Feature-Specific Metrics (Validation):\")\n",
    "    for feature, metrics in val_feature_metrics.items():\n",
    "        print(f\"  {feature}: MAE: {metrics['mae']:.4f}, RMSE: {metrics['rmse']:.4f}, R: {metrics['r2']:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ],
   "id": "7d5662d949b43fdc",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 78\u001B[0m\n\u001B[0;32m     74\u001B[0m best_val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;66;03m# Training phase\u001B[39;00m\n\u001B[1;32m---> 78\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     79\u001B[0m     train_loss, train_mae, train_rmse, train_r2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     80\u001B[0m     train_feature_metrics \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr sphere\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmae\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m     81\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr cylinder\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmae\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m     82\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml sphere\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmae\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m     83\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml cylinder\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmae\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m}}\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "342b457b37d903bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
